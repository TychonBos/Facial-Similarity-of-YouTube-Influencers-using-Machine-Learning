{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset for Siamese Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "        \n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siamese Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1707.02131.pdf?ref=https://githubhelp.com\n",
    "class SiameseNetwork128x128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork128x128, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 1, out_channels=96, kernel_size=(11,11), stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size= (3,3), stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size=(5,5), stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size= (3,3), stride=2),\n",
    "            nn.Dropout2d(p= 0.3),\n",
    "\n",
    "            nn.Conv2d(in_channels = 256, out_channels =384, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels =384, out_channels= 256, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=2),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features =43264, out_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=128, out_features=15),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    #2 forward functions\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x).cuda()\n",
    "        output = output.view(output.size()[0], -1).cuda()\n",
    "        output = self.fc1(output).cuda()\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1).cuda()\n",
    "        output2 = self.forward_once(input2).cuda()\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1707.02131.pdf?ref=https://githubhelp.com\n",
    "class SiameseNetwork32x32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork32x32, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels= 1, out_channels=96, kernel_size=(11,11), stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size= (3,3), stride=2),\n",
    "            \n",
    "            nn.Conv2d(in_channels = 96, out_channels = 256, kernel_size=(5,5), stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size= (3,3), stride=2),\n",
    "            nn.Dropout2d(p= 0.3),\n",
    "\n",
    "            nn.Conv2d(in_channels = 256, out_channels =384, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(in_channels =384, out_channels= 256, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=2),\n",
    "            nn.Dropout2d(p=0.3)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features =256, out_features=1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(in_features=128, out_features=15),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    #2 forward functions\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn1(x).cuda()\n",
    "        output = output.view(output.size()[0], -1).cuda()\n",
    "        output = self.fc1(output).cuda()\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1).cuda()\n",
    "        output2 = self.forward_once(input2).cuda()\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push models to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models 128x128 without cropping\n",
    "siamese_model128x128_2 = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_10 = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_30 = SiameseNetwork128x128().to(device)\n",
    "\n",
    "#models 32x32 without cropping\n",
    "siamese_model32x32_2 = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_10 = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_30 = SiameseNetwork32x32().to(device)\n",
    "\n",
    "#models 128x128 with dlib cropping\n",
    "siamese_model128x128_2dlib = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_10dlib = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_30dlib = SiameseNetwork128x128().to(device)\n",
    "\n",
    "#models 32x32 with dlib cropping\n",
    "siamese_model32x32_2dlib = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_10dlib = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_30dlib = SiameseNetwork32x32().to(device)\n",
    "\n",
    "#models 128x128 with haar cropping\n",
    "siamese_model128x128_2haar = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_10haar = SiameseNetwork128x128().to(device)\n",
    "siamese_model128x128_30haar = SiameseNetwork128x128().to(device)\n",
    "\n",
    "#models 32x32 with haar cropping\n",
    "siamese_model32x32_2haar = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_10haar = SiameseNetwork32x32().to(device)\n",
    "siamese_model32x32_30haar = SiameseNetwork32x32().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork32x32(\n",
       "  (cnn1): Sequential(\n",
       "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Dropout2d(p=0.3, inplace=False)\n",
       "    (7): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (10): MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout2d(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=128, out_features=15, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models 128x128 without cropping\n",
    "siamese_model128x128_2.load_state_dict(torch.load('/Siamese/Models/2train/128x128.pth'))\n",
    "siamese_model128x128_10.load_state_dict(torch.load('/Siamese/Models/10train/128x128.pth'))\n",
    "siamese_model128x128_30.load_state_dict(torch.load('/Siamese/Models/30train/128x128.pth'))\n",
    "\n",
    "#models 32x32 without cropping\n",
    "siamese_model32x32_2.load_state_dict(torch.load('/Siamese/Models/2train/32x32.pth'))\n",
    "siamese_model32x32_10.load_state_dict(torch.load('/Siamese/Models/10train/32x32.pth'))\n",
    "siamese_model32x32_30.load_state_dict(torch.load('/Siamese/Models/30train/32x32.pth'))\n",
    "\n",
    "#models 128x128 with dlib cropping\n",
    "siamese_model128x128_2dlib.load_state_dict(torch.load('/Siamese/Models/2Dlib/128x128.pth'))\n",
    "siamese_model128x128_10dlib.load_state_dict(torch.load('/Siamese/Models/10Dlib/128x128.pth'))\n",
    "siamese_model128x128_30dlib.load_state_dict(torch.load('/Siamese/Models/30Dlib/128x128.pth'))\n",
    "\n",
    "#models 32x32 with dlib cropping\n",
    "siamese_model32x32_2dlib.load_state_dict(torch.load('/2Dlib/32x32.pth'))\n",
    "siamese_model32x32_10dlib.load_state_dict(torch.load('/Siamese/Models/10Dlib/32x32.pth'))\n",
    "siamese_model32x32_30dlib.load_state_dict(torch.load('/Siamese/Models/30Dlib/32x32.pth'))\n",
    "\n",
    "#models 128x128 with haar cropping\n",
    "siamese_model128x128_2haar.load_state_dict(torch.load('/Siamese/Models/2Haar/128x128.pth'))\n",
    "siamese_model128x128_10haar.load_state_dict(torch.load('/Siamese/Models/10Haar/128x128.pth'))\n",
    "siamese_model128x128_30haar.load_state_dict(torch.load('/Siamese/Models/30Haar/128x128.pth'))\n",
    "\n",
    "#models 32x32 with haar cropping\n",
    "siamese_model32x32_2haar.load_state_dict(torch.load('/Siamese/Models/2Haar/32x32.pth'))\n",
    "siamese_model32x32_10haar.load_state_dict(torch.load('/Siamese/Models/10Haar/32x32.pth'))\n",
    "siamese_model32x32_30haar.load_state_dict(torch.load('/Siamese/Models/30Haar/32x32.pth'))\n",
    "\n",
    "#models 128x128 without cropping\n",
    "siamese_model128x128_2.eval()  \n",
    "siamese_model128x128_10.eval()  \n",
    "siamese_model128x128_30.eval() \n",
    "\n",
    "#models 32x32 without cropping\n",
    "siamese_model32x32_2.eval() \n",
    "siamese_model32x32_10.eval() \n",
    "siamese_model32x32_30.eval() \n",
    "\n",
    "#models 128x128 with dlib cropping\n",
    "siamese_model128x128_2dlib.eval() \n",
    "siamese_model128x128_10dlib.eval() \n",
    "siamese_model128x128_30dlib.eval() \n",
    "\n",
    "#models 32x32 with dlib cropping\n",
    "siamese_model32x32_2dlib.eval() \n",
    "siamese_model32x32_10dlib.eval() \n",
    "siamese_model32x32_30dlib.eval() \n",
    "\n",
    "#models 128x128 with haar cropping\n",
    "siamese_model128x128_2haar.eval() \n",
    "siamese_model128x128_10haar.eval() \n",
    "siamese_model128x128_30haar.eval()\n",
    "\n",
    "#models 32x32 with haar cropping\n",
    "siamese_model32x32_2haar.eval() \n",
    "siamese_model32x32_10haar.eval() \n",
    "siamese_model32x32_30haar.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncropped_models128x128 = [siamese_model128x128_2,  \n",
    "siamese_model128x128_10, \n",
    "siamese_model128x128_30]\n",
    "\n",
    "uncropped_models32x32 = [\n",
    "siamese_model32x32_2,\n",
    "siamese_model32x32_10,\n",
    "siamese_model32x32_30]\n",
    "\n",
    "dlib_models128x128 = [siamese_model128x128_2dlib,\n",
    "siamese_model128x128_10dlib, \n",
    "siamese_model128x128_30dlib]\n",
    "\n",
    "dlib_models32x32 = [\n",
    "siamese_model32x32_2dlib,\n",
    "siamese_model32x32_10dlib, \n",
    "siamese_model32x32_30dlib]\n",
    "\n",
    "haar_models128x128 = [siamese_model128x128_2haar,\n",
    "siamese_model128x128_10haar, \n",
    "siamese_model128x128_30haar]\n",
    "\n",
    "haar_models32x32 = [\n",
    "siamese_model32x32_2haar,\n",
    "siamese_model32x32_10haar, \n",
    "siamese_model32x32_30haar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influencers = ['Airrack', 'BellaPoarch', 'Larry', 'LexiHensler', 'LoganPaul', 'MarkRober', 'MrBeast', 'PiersonWodzynski', 'Preston', 'Quackity', 'RosannaPansino', 'RyanTrahan', 'SoffieDossi', 'ZachKing', 'Zhc' ]\n",
    "male_if = ['Airrack', 'Larry', 'LoganPaul', 'MarkRober', 'MrBeast', 'Preston', 'Quackity','RyanTrahan','ZachKing', 'Zhc']\n",
    "female_if = ['BellaPoarch',  'LexiHensler','PiersonWodzynski','RosannaPansino','SoffieDossi']\n",
    "folder = '/Youtube1video/'\n",
    "\n",
    "names = [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))] #choose random names from people that appear in 1 video\n",
    "random.seed(10)\n",
    "choice = random.sample(names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset to load images from specific directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagefolder pytorch select only images from specific folder\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.image_paths = glob.glob(os.path.join(path, '*.jpg'))\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = Image.open(self.image_paths[index])\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(person1, person2, model, size):\n",
    "    \n",
    "    dissim = []\n",
    "\n",
    "    query_folder = 'C:/Users/Tychon Bos/Documents/Youtube1person/' + person1 + '/'\n",
    "    reference_folder = 'C:/Users/Tychon Bos/Documents/Youtube1person/' + person2 + '/'\n",
    "    \n",
    "    dataset_query = MyDataset(query_folder, transform = transforms.Compose([transforms.Resize(size), transforms.Grayscale(num_output_channels=1) ,transforms.ToTensor()]))\n",
    "    dataset_reference = MyDataset(reference_folder, transform=transforms.Compose([transforms.Resize(size), transforms.Grayscale(num_output_channels=1) ,transforms.ToTensor()]))\n",
    "\n",
    "    dataset_query_loader = DataLoader(dataset_query,num_workers=0,batch_size=1,shuffle=True) #left image changes\n",
    "    dataset_reference_loader = DataLoader(dataset_reference,num_workers=0,batch_size=1,shuffle=False) #right image order does not change\n",
    "\n",
    "    dataiter_dataset_query_loader = iter(dataset_query_loader)\n",
    "    dataiter_dataset_reference_loader = iter(dataset_reference_loader)\n",
    "\n",
    "    #labels are person names; if person names not the same --> 1 else 0\n",
    "    #calculate average dissimilarity score between the same persons for every model with nested for loop so that left image also changes    \n",
    "\n",
    "    for i in range(1):\n",
    "        x0 = next(dataiter_dataset_query_loader)\n",
    "    \n",
    "        for j in range(30):\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    x1 = next(dataiter_dataset_reference_loader)\n",
    "                    concatenated = torch.cat((x0,x1),0)\n",
    "                    \n",
    "                    output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                    z = torch.tensor(euclidean_distance.item())\n",
    "                    dissim.append(torch.sigmoid(z).item())\n",
    "                    #imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(torch.sigmoid(z)))\n",
    "            except StopIteration:\n",
    "                pass\n",
    "    \n",
    "    avg = round(sum(dissim)/len(dissim), 4)\n",
    "    #print(avg)\n",
    "    overall.append(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average uncropped_models128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_uncropped_models128x128 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    print(influencer1, influencer2)\n",
    "    input_size = (128,128)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "\n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(uncropped_models128x128)):\n",
    "        overall = []\n",
    "        current_model = uncropped_models128x128[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    all_avg_uncropped_models128x128.append(overall_avg)\n",
    "    print(all_avg_uncropped_models128x128)\n",
    "    print()\n",
    "x = sum(all_avg_uncropped_models128x128)/len(influencers)\n",
    "print('Overall average for all uncropped 128x128 models: {} '.format(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average haar_models128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_haar_models128x128 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    input_size = (128,128)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "\n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(haar_models128x128)):\n",
    "        overall = []\n",
    "        current_model = haar_models128x128[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    all_avg_haar_models128x128.append(overall_avg)\n",
    "    print(all_avg_haar_models128x128)\n",
    "    print()\n",
    "x = sum(all_avg_haar_models128x128)/len(influencers)\n",
    "print('Overall average for all haar 128x128 models: {} '.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average dlib_models128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_dlib_models128x128 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    input_size = (128,128)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "\n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(dlib_models128x128)):\n",
    "        overall = []\n",
    "        current_model = dlib_models128x128[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    print()\n",
    "    all_avg_dlib_models128x128.append(overall_avg)\n",
    "    print(all_avg_dlib_models128x128)\n",
    "x = sum(all_avg_dlib_models128x128)/len(influencers)\n",
    "print('Overall average for all dlib 128x128 models: {} '.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average uncropped_models32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_uncropped_models32x32 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    input_size = (32,32)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "\n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(uncropped_models32x32)):\n",
    "        overall = []\n",
    "        current_model = uncropped_models32x32[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and uncropped models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    all_avg_uncropped_models32x32.append(overall_avg)\n",
    "    print(all_avg_uncropped_models32x32)\n",
    "    print()\n",
    "x = sum(all_avg_uncropped_models32x32)/len(influencers)\n",
    "print('Overall average for all uncropped 32x32 models: {} '.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average haar_models32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_haar_models32x32 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    input_size = (32,32)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "\n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(haar_models32x32)):\n",
    "        overall = []\n",
    "        current_model = haar_models32x32[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and haar models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    all_avg_haar_models32x32.append(overall_avg)\n",
    "    print(all_avg_haar_models32x32)\n",
    "    print()\n",
    "x = sum(all_avg_haar_models32x32)/len(influencers)\n",
    "print('Overall average for all haar 32x32 models: {} '.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all average dlib_models32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_dlib_models32x32 = []\n",
    "for name1, name2 in zip(choice, choice):\n",
    "    influencer1 = name1\n",
    "    influencer2 = name2\n",
    "    input_size = (32,32)\n",
    "    count = 0\n",
    "    final_avg = []\n",
    "    \n",
    "    if influencer1 != influencer2:\n",
    "        raise ValueError(\"Names are not the same!\")\n",
    "    \n",
    "    for model in range(len(dlib_models32x32)):\n",
    "        overall = []\n",
    "        current_model = dlib_models32x32[model]\n",
    "        for i in range(30):\n",
    "            test(influencer1, influencer2, current_model, input_size)\n",
    "\n",
    "        avg_overall = round(sum(overall)/len(overall),4)\n",
    "        final_avg.append(avg_overall)\n",
    "        \n",
    "        if len(overall) != 30:\n",
    "            raise ValueError('Overall length is not 30!')\n",
    "        \n",
    "        if count == 0:\n",
    "            print(\"The average overall for {} to {} is: {} with 2 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        elif count == 1:\n",
    "            print(\"The average overall for {} to {} is: {} with 10 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "        else:\n",
    "            print(\"The average overall for {} to {} is: {} with 30 training images and dlib models\".format(influencer1, influencer2, avg_overall))\n",
    "            count +=1\n",
    "            \n",
    "    print(final_avg)\n",
    "    overall_avg = round(sum(final_avg)/len(final_avg),4)\n",
    "    print('Overall average for {} to {} is: {}'.format(influencer1, influencer2, overall_avg))\n",
    "    all_avg_dlib_models32x32.append(overall_avg)\n",
    "    print(all_avg_dlib_models32x32)\n",
    "    print()\n",
    "x = sum(all_avg_dlib_models32x32)/len(influencers)\n",
    "print('Overall average for all 32x32 dlib models: {} '.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot scores person to person "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_uncropped_models128x128 = round(sum(all_avg_uncropped_models128x128)/len(choice),4)\n",
    "print(total_uncropped_models128x128)\n",
    "\n",
    "total_haar_models128x128 = round(sum(all_avg_haar_models128x128)/len(choice),4)\n",
    "print(total_haar_models128x128)\n",
    "\n",
    "total_dlib_models128x128 = round(sum(all_avg_dlib_models128x128)/len(choice),4)\n",
    "print(total_dlib_models128x128)\n",
    "\n",
    "total_uncropped_models32x32 = round(sum(all_avg_uncropped_models32x32)/len(choice),4)\n",
    "print(total_uncropped_models32x32)\n",
    "\n",
    "total_haar_models32x32 = round(sum(all_avg_haar_models32x32)/len(choice),4)\n",
    "print(total_haar_models32x32)\n",
    "\n",
    "total_dlib_models32x32 = round(sum(all_avg_dlib_models32x32)/len(choice),4)\n",
    "print(total_dlib_models32x32)\n",
    "\n",
    "names = ['128x128 uncropped', '128x128 haar', '128x128 dlib', '32x32 uncropped', '32x32 haar', '32x32 dlib']\n",
    "values = [total_uncropped_models128x128, total_haar_models128x128, total_dlib_models128x128, total_uncropped_models32x32, total_haar_models32x32, total_dlib_models32x32]\n",
    "\n",
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(i,y[i],y[i], ha='center', fontstyle='oblique')\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.ylim([0.5, 0.8])\n",
    "plt.bar(names, values, align='center', width=0.6, color=(0.2, 0.4, 0.6, 0.6), alpha=1, edgecolor='black')\n",
    "addlabels(names, values)\n",
    "plt.title('Average dissimilarity score computed for each influencer to itself averaged over 2, 10 and 30 images used in training for each model ')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that plots the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary classification\n",
    "\n",
    "Add manually paths for the query_folder, reference_folder and save_path variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification(model, input_size, runs, treshold):\n",
    "    # model: which model to use\n",
    "    # input size: tuple which contains the input size\n",
    "    # runs: how many runs to use\n",
    "    # treshold: binary classification threshold\n",
    "    # generate 30 pairs of people and for each pair compute 30 dissimilarity scores\n",
    "    treshold = treshold\n",
    "    labellist = []\n",
    "    scorelist = []\n",
    "    amount_of_images_pairs = 30 #30\n",
    "    epochs = 30 #30\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    count = 0\n",
    "    \n",
    "    for num in range(runs):\n",
    "\n",
    "        for i in range(epochs): #30\n",
    "            name1 = random.choice(influencers)\n",
    "            #name1 = random.choice(choice)\n",
    "            #name1 = random.choice(female_if)\n",
    "            #we need to make sure approx 50% of images are in the same class\n",
    "            should_get_same_class = random.randint(0,1) \n",
    "            if should_get_same_class:\n",
    "                while True:\n",
    "                    #keep looping till the same class image is found\n",
    "                    name2 = random.choice(influencers) \n",
    "                    #name2 = random.choice(choice) \n",
    "                    #name2 = random.choice(female_if)\n",
    "                    if name1==name2:\n",
    "                        for n in range(amount_of_images_pairs):\n",
    "                            labellist.append(0) #if names are the same append 0\n",
    "                        break\n",
    "            else:\n",
    "                while True:\n",
    "                    #keep looping till a different class image is found\n",
    "                    name2 = random.choice(influencers) \n",
    "                    #name2 = random.choice(choice) \n",
    "                    #name2 = random.choice(female_if)\n",
    "                    if name1 !=name2:\n",
    "                        for n in range(amount_of_images_pairs):\n",
    "                            labellist.append(1) #if names are different append 1\n",
    "                        break\n",
    "\n",
    "            query_folder = '/influencer_data/' + name1 + '/'\n",
    "            reference_folder = '/influencer_data/' + name2 + '/'\n",
    "            \n",
    "            #query_folder = '/Youtube1video/' + name1 + '/'\n",
    "            #reference_folder = '/Youtube1video/' + name2 + '/'\n",
    "            dataset_query = MyDataset(query_folder, transform = transforms.Compose([transforms.Resize(input_size), transforms.Grayscale(num_output_channels=1) ,transforms.ToTensor()]))\n",
    "            dataset_reference = MyDataset(reference_folder, transform=transforms.Compose([transforms.Resize(input_size), transforms.Grayscale(num_output_channels=1) ,transforms.ToTensor()]))\n",
    "\n",
    "            dataset_query_loader = DataLoader(dataset_query,num_workers=0,batch_size=1,shuffle=False) \n",
    "            dataset_reference_loader = DataLoader(dataset_reference,num_workers=0,batch_size=1,shuffle=False) \n",
    "\n",
    "            dataiter_dataset_query_loader = iter(dataset_query_loader)\n",
    "            dataiter_dataset_reference_loader = iter(dataset_reference_loader)\n",
    "            \n",
    "            temp_label = 5\n",
    "            \n",
    "            for i in range(1):\n",
    "                x0 = next(dataiter_dataset_query_loader)\n",
    "            \n",
    "                for j in range(amount_of_images_pairs):\n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            if name1 == name2: #if names are the same (positive pairs) set temp_label variable to 0\n",
    "                                temp_label = 0\n",
    "                                # print(temp_label)\n",
    "                                x1 = next(dataiter_dataset_reference_loader)\n",
    "                                concatenated = torch.cat((x0,x1),0)\n",
    "                                \n",
    "                                output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                \n",
    "                                z = torch.tensor(euclidean_distance.item())\n",
    "                                sigmoid_value = round(torch.sigmoid(z).item(),4)\n",
    "                                \n",
    "                                if sigmoid_value < treshold:\n",
    "                                    tp +=1\n",
    "                                    grid = torchvision.utils.make_grid(concatenated,nrow=x0.size(0),padding=0)\n",
    "\n",
    "                                    npimg = grid.detach().numpy() # to numpy array\n",
    "\n",
    "                                    npimg = (npimg * 255).astype(np.uint8)\n",
    "                                    \n",
    "                                    fig, ax = plt.subplots(figsize = (12,8))\n",
    "                                    ax.axis(\"off\")\n",
    "                                    output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                    #scores.append(euclidean_distance.item())\n",
    "\n",
    "                                    z = torch.tensor(euclidean_distance.item())\n",
    "                                    z = torch.sigmoid(z)\n",
    "                                    z = z.item()\n",
    "                                    z= round(z,4)\n",
    "\n",
    "                                    ax.set_title('Dissimilarity: ' + str(z), fontsize=45)\n",
    "                                    imshow(torchvision.utils.make_grid(concatenated))\n",
    "\n",
    "                                    savepath = \"save here/\" + str(count) + '.jpg'\n",
    "                                    count +=1\n",
    "                                    fig.savefig(savepath)\n",
    "                                    \n",
    "                                else:\n",
    "                                    fn +=1\n",
    "                                    grid = torchvision.utils.make_grid(concatenated,nrow=x0.size(0),padding=0)\n",
    "\n",
    "                                    npimg = grid.detach().numpy() # to numpy array\n",
    "\n",
    "                                    npimg = (npimg * 255).astype(np.uint8)\n",
    "                                    \n",
    "                                    fig, ax = plt.subplots(figsize = (12,8))\n",
    "                                    ax.axis(\"off\")\n",
    "                                    output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                    #scores.append(euclidean_distance.item())\n",
    "\n",
    "                                    z = torch.tensor(euclidean_distance.item())\n",
    "                                    z = torch.sigmoid(z)\n",
    "                                    z = z.item()\n",
    "                                    z= round(z,4)\n",
    "\n",
    "                                    ax.set_title('Dissimilarity: ' + str(z), fontsize=45)\n",
    "                                    imshow(torchvision.utils.make_grid(concatenated))\n",
    "\n",
    "                                    savepath = \"save here/\" + str(count) + '.jpg'\n",
    "                                    count +=1\n",
    "                                    fig.savefig(savepath)\n",
    "                                    \n",
    "                                scorelist.append(sigmoid_value)\n",
    "                            \n",
    "                            elif name1 != name2: #if names are different (negative pairs) set temp_label variable to 1\n",
    "                                temp_label = 1\n",
    "                                \n",
    "                                x1 = next(dataiter_dataset_reference_loader)\n",
    "                                concatenated = torch.cat((x0,x1),0)\n",
    "                                \n",
    "                                output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                \n",
    "                                z = torch.tensor(euclidean_distance.item())\n",
    "                                sigmoid_value = round(torch.sigmoid(z).item(),4)\n",
    "                                \n",
    "                                if sigmoid_value > treshold:\n",
    "                                    tn +=1\n",
    "                                    grid = torchvision.utils.make_grid(concatenated,nrow=x0.size(0),padding=0)\n",
    "\n",
    "                                    npimg = grid.detach().numpy() # to numpy array\n",
    "\n",
    "                                    npimg = (npimg * 255).astype(np.uint8)\n",
    "                                    \n",
    "                                    fig, ax = plt.subplots(figsize = (12,8))\n",
    "                                    ax.axis(\"off\")\n",
    "                                    output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                    #scores.append(euclidean_distance.item())\n",
    "\n",
    "                                    z = torch.tensor(euclidean_distance.item())\n",
    "                                    z = torch.sigmoid(z)\n",
    "                                    z = z.item()\n",
    "                                    z= round(z,4)\n",
    "\n",
    "                                    ax.set_title('Dissimilarity: ' + str(z), fontsize=45)\n",
    "                                    imshow(torchvision.utils.make_grid(concatenated))\n",
    "\n",
    "                                    savepath = \"save here/\" + str(count) + '.jpg'\n",
    "                                    count +=1\n",
    "                                    fig.savefig(savepath)\n",
    "                                else:\n",
    "                                    fp +=1 \n",
    "                                    \n",
    "                                    grid = torchvision.utils.make_grid(concatenated,nrow=x0.size(0),padding=0)\n",
    "\n",
    "                                    npimg = grid.detach().numpy() # to numpy array\n",
    "\n",
    "                                    npimg = (npimg * 255).astype(np.uint8)\n",
    "                                    \n",
    "                                    fig, ax = plt.subplots(figsize = (12,8))\n",
    "                                    ax.axis(\"off\")\n",
    "                                    output1,output2 = model(Variable(x0).to(device),Variable(x1).to(device))\n",
    "                                    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "                                    #scores.append(euclidean_distance.item())\n",
    "\n",
    "                                    z = torch.tensor(euclidean_distance.item())\n",
    "                                    z = torch.sigmoid(z)\n",
    "                                    z = z.item()\n",
    "                                    z= round(z,4)\n",
    "\n",
    "                                    ax.set_title('Dissimilarity: ' + str(z), fontsize=45)\n",
    "                                    imshow(torchvision.utils.make_grid(concatenated))\n",
    "                                    \n",
    "                                    savepath = \"save here/\" + str(count) + '.jpg'\n",
    "                                    count +=1\n",
    "                                    fig.savefig(savepath) \n",
    "                                    \n",
    "                                scorelist.append(sigmoid_value)\n",
    "                            \n",
    "                            #imshow(torchvision.utils.make_grid(concatenated),'Dissimilarity: {:.2f}'.format(torch.sigmoid(z)))\n",
    "                            \n",
    "                    except StopIteration:\n",
    "                        pass\n",
    "\n",
    "        total_img = epochs * amount_of_images_pairs\n",
    "        accuracy = round((tp + tn)/ total_img,2)\n",
    "        precision = round(tp / (tp + fp),4)\n",
    "        recall = round(tp / (tp + fn),4)\n",
    "        \n",
    "    labellist = np.array(labellist)\n",
    "    scorelist = np.array(scorelist)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(labellist, scorelist)\n",
    "    print('roc auc score: ', roc_auc_score(labellist, scorelist))\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    print(\"Threshold value is:\", optimal_threshold)\n",
    "    plot_roc_curve(fpr, tpr)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labellist, scorelist)\n",
    "    auc = roc_auc_score(labellist, scorelist)\n",
    "    plt.plot(fpr,tpr,label= \"auc=\"+str(auc))\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()  \n",
    "    \n",
    "    print('precision: ', precision)\n",
    "    print('recall: ', recall)\n",
    "    \n",
    "    #Generate the confusion matrix\n",
    "    prob_to_label = []\n",
    "    for score in scorelist:\n",
    "        if score < treshold:\n",
    "            prob_to_label.append(0)\n",
    "        else:\n",
    "            prob_to_label.append(1)\n",
    "        \n",
    "    cf_matrix = confusion_matrix(labellist, prob_to_label)\n",
    "    \n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                        cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "            zip(group_names,group_counts,group_percentages)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n",
    "\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ')\n",
    "\n",
    "    ax.xaxis.set_ticklabels(['False','True'])\n",
    "    ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(tp, fn, tn, fp)\n",
    "    \n",
    "    return 'Average accuracy score is: {} over {} runs'.format(round(accuracy/runs, 2), runs)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e0031dbb810717f23483bfe6bab95bd9ab8defa9c07b5d9d514062305268423"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
